# Mathematics Modeling Note C-6  
## 随机森林算法
***机器学习中的两种任务：分类与回归***  
***回归是对连续值进行预测***   
***分类是对离散值进行预测***   
***随机森林算法两种任务都可以完成***   
***随机森林算法是一种机器学习算法***   

基本思想：集成多个分类器  
集成学习：将多个分类器组合以实现预测效果更好的集成分类器  
集成算法：Bagging，Boosting，Stacking  
其中随机森林算法采用Bagging的思想，以决策树为基本单元，通过集成大量决策树构成随机森林  

### Bagging：
1. 每次有放回地从训练集中取出 n 个训练样本，组成新的训练集；

2. 利用新的训练集，训练得到M个子模型；

3. 对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；  
4. 对于回归问题，采用简单的平均方法得到预测值。  

树的构建：样本与特征  
样本：在训练集T中有N个样本，有放回地随机选择N个样本（可能选不够N个样本）作为决策树节点处样本  
特征：在d个特征中选择k（k << d）个特征  

每个样本有d个属性，当决策树节点需要分裂是，随机从d个属性中选取出k个属性，然后采用某种策略选出一个属性作为该节点的分裂属性  

算法流程：![](./picture/1706455836487.png)  

信息熵：度量样本集合纯度的指标  
$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$,D为样本集合，k为第k类样本，$p_k$为第k类样本所占比例。Ent(D)越小，D的纯度越高  

信息增益：通过得知特证X的信息从而使得类k的不确定性减小的程度，即信息增益越大，第k类的纯度提升越大  
$g(D, A)=H(D)-H(D/A)$,其中A为特征，H(D)为样本集合D的经验熵，H(D/A)为经验条件熵  

ID3决策树通过信息增益选择特征属性，但容易造成过拟合；C4.5改进了ID3,采用信息增益比选择特征  

基尼指数(Gini index):  
$Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{D})^2$,其中$C_k$为第k类样本子集，K为类的个数  

随机森林算法采用基尼指数(Gini index)选择特征属性，也可以用袋外误差(oob error)

决策树的生成：递归  

剪枝：防止过拟合现象，简化决策树（学习时过度准确分类训练数据，测试数据分类不准确，构造出的决策树复杂），包括损失函数(loss function),代价函数(cost function)

随机森林的完整操作过程梳理如下：

（1）首先，从将数据集分为训练集和测试集。

（2）第一个决策树的产生过程如下：

①使用Bootstrap方法从训练集中抽取出一个数据集用于生成第1个决策树。Bootstrap抽样方法随机地从训练集中抽取数据形成新的数据集，并且允许重复抽取同一样本，抽取出的数据集大小等于训练集大小。

②假设有n个变量，每次随机选择2个变量，判断其中的哪个变量更适合做节点，判断依据可以是信息增益、gini指标。通过这种方法先确定决策树的根节点。然后，在剩下的n-1个变量中，继续随机选2个变量，判断谁更适合作为内部节点，并确定内部节点。不断循环，直到生成一棵完整的树。

每次随机选择的变量数可以是3个、4个甚至更多，但如果每次都随机选两个，决策树的生成会更快。

后面的决策树都是循环第①步和第②步产生的。一般来说，随机森林中的决策树棵数为100棵，你也可以生成更多的树。

（3）使用随机森林对新样本（来自测试集）进行分类预测。

假设随机森林中有100棵决策树，那么，我们需要将新样本数据输入到100棵树中，然后，看每棵树返回的分类结果是什么，如果是分类问题，则根据投票原则，少数服从多数来确定新样本最终的类别。如果是回归问题，则取100棵树预测值的均值作为最终的预测值。

如：假设新样本可能有两种类别取值：0和1，使用规模为100的随机森林模型发现，有90棵树预测的类别是1，10棵树预测的结果是0，那么，少数服从多数，新样本的类别会判断为1。
## 网格搜索
一种用于确定机器学习参数的方法  
穷举参数的所有组合，根据评价指标选定最佳组合  
1. 参数空间网格化：将各个参数取值范围离散化，构建网格参数空间  
2. 创建参数组合：生成参数网络中所有组合，通常使用笛卡尔乘积
3. 穷举搜索：遍历代入，并通过交叉印证等方法检验模型性能  
4. 选择最佳参数组合  

## 随机森林回归
1. 随机有放回地抽取样本，构建与原训练集数量相同的新训练集D'  
2. 选择特征
3. 构建回归树T


